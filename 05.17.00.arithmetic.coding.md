### Arithmetic Coding

Suppose we have 4 symbols (for example, the inter mode can be NEW, NEAREST,
NEAR, ZERO), that we wish to encode. If these are all equally likely then
encoding each with 2 bits would be fine:

| MODE      | BITS      |
|:---------:|:---------:|
| NEW       | 00        |
| NEAR      | 01        |
| NEAREST   | 10        |
| ZERO      | 11        |

However, suppose ZERO happened 50% of the time, NEAREST 25%, and NEAR/NEW
12.5%. In this case we could use the variable length codes:

| MODE      | BITS      |
|:---------:|:---------:|
| NEW       | 000       |
| NEAR      | 001       |
| NEAREST   | 01        |
| ZERO      | 1         |

This scheme would now give fewer bits on average than the uniform encoding
scheme.

Now suppose that ZERO happened 90% of the time. **Arithmetic coding** provides
a way to allow us to effectively use a fraction of a bit in this case.

At the lowest level AV1 contains a **boolean decoder** which decodes one
boolean value (0 or 1) at a time given an input containing the estimated
probability of the value. If the boolean value is much more likely to be a 1
than a 0 (or the other way around), then it can be faithfully coded using less
than 1 bit per boolean value on average using an arithmetic coder.

The boolean decoder works using a small set of unsigned 16-bit integers and an
unsigned 16-bit multiplication operation.
